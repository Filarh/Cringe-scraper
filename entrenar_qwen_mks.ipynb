{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Filarh/Cringe-scraper/blob/main/entrenar_qwen_mks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUyEBsGmtE17"
      },
      "outputs": [],
      "source": [
        "!pip install -qq --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "!pip install -qq sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install -qq --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "1tFATxBPtpdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the base model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B\",\n",
        "    max_seq_length = 2048,   # Define context length\n",
        "    load_in_4bit = True,     # Enable 4-bit quantization\n",
        "    # token = \"hf_...\",      # Add your token if using a gated model\n",
        ")"
      ],
      "metadata": {
        "id": "IE4SKZvhtuyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters to the model\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # LoRA rank (higher rank = more parameters, potentially better fit but more memory)\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # Target attention and MLP layers\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Scaling factor (often set to r or 2*r)\n",
        "    lora_dropout = 0, # Dropout probability for LoRA layers\n",
        "    bias = \"none\",    # Fine-tuning bias terms ('none' is often optimal)\n",
        "    # Use Unsloth's gradient checkpointing for memory saving\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False, # Rank Stable LoRA (optional)\n",
        "    loftq_config = None, # LoftQ initialization (optional)\n",
        ")"
      ],
      "metadata": {
        "id": "lcAoIsudtvSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Ruta a tu archivo original ya cargado\n",
        "file_path = \"/content/postexpandedv2.jsonl\"\n",
        "\n",
        "# Leer y limpiar las entradas\n",
        "data = []\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            entry = json.loads(line.strip())\n",
        "            # A√±adir <think></think> si no est√° presente\n",
        "            if \"<think>\" not in entry[\"output\"].lower():\n",
        "                entry[\"output\"] = \"<think></think> \" + entry[\"output\"]\n",
        "            data.append(entry)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "# Funci√≥n para detectar contenido real dentro de <think>...</think>\n",
        "def has_thinking_content(text):\n",
        "    match = re.search(r\"<think>\\s*(.*?)\\s*</think>\", text, re.DOTALL | re.IGNORECASE)\n",
        "    return bool(match and match.group(1).strip())\n",
        "\n",
        "# Dividir seg√∫n si tienen razonamiento real o no\n",
        "reasoning_data = [e for e in data if has_thinking_content(e[\"output\"])]\n",
        "non_reasoning_data = [e for e in data if not has_thinking_content(e[\"output\"])]\n",
        "\n",
        "# Crear datasets\n",
        "reasoning_dataset = Dataset.from_list(reasoning_data)\n",
        "non_reasoning_dataset = Dataset.from_list(non_reasoning_data)\n",
        "\n",
        "# Mostrar ejemplos\n",
        "print(\"‚úÖ Razonamiento detectado:\", len(reasoning_dataset))\n",
        "print(\"üü° Sin razonamiento:\", len(non_reasoning_dataset))\n",
        "print(\"\\nEjemplo razonamiento:\")\n",
        "print(reasoning_dataset[0])\n",
        "\n",
        "if len(non_reasoning_dataset) > 0:\n",
        "    print(\"\\nEjemplo sin razonamiento:\")\n",
        "    print(non_reasoning_dataset[0])\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No se encontraron ejemplos sin razonamiento.\")\n"
      ],
      "metadata": {
        "id": "kxIC4AhAuCnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_reasoning_conversation(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    problems     = examples[\"input\"]\n",
        "    solutions    = examples[\"output\"]\n",
        "\n",
        "    conversations = []\n",
        "    for instruction, problem, solution in zip(instructions, problems, solutions):\n",
        "        conversations.append([\n",
        "            {\"role\": \"system\",    \"content\": instruction},\n",
        "            {\"role\": \"user\",      \"content\": problem},\n",
        "            {\"role\": \"assistant\", \"content\": solution},\n",
        "        ])\n",
        "    return { \"conversations\": conversations }\n"
      ],
      "metadata": {
        "id": "aAlBaKrmuKX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_non_reasoning_conversation(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "\n",
        "    conversations = []\n",
        "    for instruction, input_text, output_text in zip(instructions, inputs, outputs):\n",
        "        conversations.append([\n",
        "            {\"role\": \"system\",    \"content\": instruction},\n",
        "            {\"role\": \"user\",      \"content\": input_text},\n",
        "            {\"role\": \"assistant\", \"content\": output_text},\n",
        "        ])\n",
        "    return { \"conversations\": conversations }\n"
      ],
      "metadata": {
        "id": "nr-THEFE3Npu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar la plantilla de conversaci√≥n\n",
        "reasoning_formatted_texts = tokenizer.apply_chat_template(\n",
        "    reasoning_dataset.map(generate_reasoning_conversation, batched=True)[\"conversations\"],\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# Mostrar el primer resultado formateado\n",
        "print(\"First formatted Reasoning Row:\")\n",
        "print(reasoning_formatted_texts[0])\n"
      ],
      "metadata": {
        "id": "Qz5SSqZ9uWsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_formatted_texts = tokenizer.apply_chat_template(\n",
        "    non_reasoning_dataset.map(generate_non_reasoning_conversation, batched=True)[\"conversations\"],\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "print(\"\\nFirst formatted Non-Reasoning Row:\")\n",
        "print(non_reasoning_formatted_texts[0])\n"
      ],
      "metadata": {
        "id": "0Z7KQOk-3iZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "cWQ9tZK4ueNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define desired chat data percentage\n",
        "chat_percentage = 0.75 # Aim for 75% chat data\n",
        "# Convert to Pandas Series for easier sampling\n",
        "reasoning_series = pd.Series(reasoning_formatted_texts)\n",
        "non_reasoning_series = pd.Series(non_reasoning_formatted_texts)\n",
        "# Sample non-reasoning data based on the desired ratio relative to reasoning data\n",
        "# Calculate how many non-reasoning samples we need\n",
        "num_non_reasoning_samples = int(len(reasoning_series) * (chat_percentage / (1.0 - chat_percentage)))\n",
        "# Ensure we don't request more samples than available\n",
        "num_non_reasoning_samples = min(num_non_reasoning_samples, len(non_reasoning_series))\n",
        "\n",
        "print(f\"Using {len(reasoning_series)} reasoning samples.\")\n",
        "print(f\"Sampling {num_non_reasoning_samples} non-reasoning samples.\")"
      ],
      "metadata": {
        "id": "Zm8178KOul1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_subset = non_reasoning_series.sample(\n",
        "    n = num_non_reasoning_samples,\n",
        "    random_state = 2407, # for reproducibility\n",
        ")\n",
        "\n",
        "# Combine the datasets\n",
        "combined_series = pd.concat([reasoning_series, non_reasoning_subset])\n",
        "combined_series.name = \"text\" # The SFTTrainer expects this column name\n",
        "\n",
        "# Convert back to Hugging Face Dataset and shuffle\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(combined_series))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)\n",
        "\n",
        "print(f\"\\nFinal Combined Dataset size: {len(combined_dataset)}\")\n",
        "print(\"Example entry from combined dataset:\")\n",
        "print(combined_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "m6puGGT9uoXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig"
      ],
      "metadata": {
        "id": "6SZfCiHnur2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sftconfig = SFTConfig(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Effective batch size = 2 * 4 = 8\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,                 # Short run for demonstration; set to None for full epochs\n",
        "        # num_train_epochs = 1,         # Alternatively, train for 1 full epoch\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not torch.cuda.is_bf16_supported(), # Use bf16 if available, else fp16\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",           # Use 8-bit AdamW optimizer\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",         # Directory to save checkpoints\n",
        "        report_to = \"none\",             # Disable external reporting (like WandB) for this example\n",
        "    )"
      ],
      "metadata": {
        "id": "ME33Pe3Ru3uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    dataset_text_field = \"text\", # Column name we created\n",
        "    max_seq_length = 2048,      # Should match model loading\n",
        "    args = sftconfig\n",
        ")"
      ],
      "metadata": {
        "id": "paYZ9VSGuxrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "print(\"Starting training...\")\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Training finished.\")\n",
        "# You can print training stats if needed\n",
        "# print(trainer_stats)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B0WrXdzcu9gy",
        "outputId": "82ae33d8-0c0e-4e2f-d8dd-403309f0e74a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:53, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.802400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.291200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.551100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.557300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.630100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.116500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.990300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.835100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.558400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.070500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.434700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.218700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.897500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.846800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.080400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.311700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.423200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.894900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.227100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.796700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.551200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.732200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.817400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.622300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.761300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.801200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"Solve (x + 2)^2 = 0.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "GwA-J9GUwZFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the prompt, explicitly DISABLING thinking mode\n",
        "text_input_no_think = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Crucial for generation\n",
        "    enable_thinking = False,      # *** Disable thinking ***\n",
        ")\n",
        "\n",
        "\n",
        "print(\"--- Non-Thinking Inference ---\")\n",
        "print(\"Formatted Input:\\n\", text_input_no_think)"
      ],
      "metadata": {
        "id": "tuVx03kmwxxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response using parameters suitable for non-thinking/chat\n",
        "inputs = tokenizer(text_input_no_think, return_tensors = \"pt\").to(\"cuda\")\n",
        "streamer_no_think = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens = 256,\n",
        "    temperature = 0.7, # Recommended for chat\n",
        "    top_p = 0.8,       # Recommended for chat\n",
        "    top_k = 20,\n",
        "    streamer = streamer_no_think,\n",
        "    eos_token_id = tokenizer.eos_token_id # Ensure generation stops properly\n",
        ")\n",
        "print(\"\\n-----------------------------\")"
      ],
      "metadata": {
        "id": "CpSr2kMCw-ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Thinking Inference:**"
      ],
      "metadata": {
        "id": "8pw1a9dgxLqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the prompt, explicitly ENABLING thinking mode\n",
        "text_input_think = tokenizer.apply_chat_template(\n",
        "    messages, # Same user message\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True,\n",
        "    enable_thinking = True,       # *** Enable thinking ***\n",
        ")\n",
        "\n",
        "print(\"--- Thinking Inference ---\")\n",
        "print(\"Formatted Input:\\n\", text_input_think)"
      ],
      "metadata": {
        "id": "_O3TIo-nxAnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response using parameters suitable for thinking/reasoning\n",
        "inputs_think = tokenizer(text_input_think, return_tensors = \"pt\").to(\"cuda\")\n",
        "streamer_think = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(\n",
        "    **inputs_think,\n",
        "    max_new_tokens = 1024, # Allow more tokens for reasoning steps\n",
        "    temperature = 0.6,   # Recommended for reasoning\n",
        "    top_p = 0.95,        # Recommended for reasoning\n",
        "    top_k = 20,\n",
        "    streamer = streamer_think,\n",
        "    eos_token_id = tokenizer.eos_token_id # Ensure generation stops properly\n",
        ")\n",
        "print(\"\\n-----------------------------\")"
      ],
      "metadata": {
        "id": "yfDCj_bPxTkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save LoRA adapters locally\n",
        "model.save_pretrained(\"qwen3_0.6b_reasoning_chat_lora\")\n",
        "tokenizer.save_pretrained(\"qwen3_0.6b_reasoning_chat_lora\")\n",
        "\n",
        "print(\"LoRA adapters saved locally to 'qwen3_0.6b_reasoning_chat_lora'\")\n",
        "\n",
        "# Optional: Push to Hugging Face Hub\n",
        "# model.push_to_hub(\"your_username/qwen3_14b_reasoning_chat_lora\", token=\"YOUR_HF_TOKEN\")\n",
        "# tokenizer.push_to_hub(\"your_username/qwen3_14b_reasoning_chat_lora\", token=\"YOUR_HF_TOKEN\")\n",
        "\n",
        "# To load these adapters later:\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"qwen3_14b_reasoning_chat_lora\", # Path to saved adapters\n",
        "#     load_in_4bit = True,\n",
        "# )"
      ],
      "metadata": {
        "id": "AOe-YIjSxbL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "erNXKWmWx-nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}