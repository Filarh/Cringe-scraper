{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Filarh/Cringe-scraper/blob/main/run_in_colab_traducido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NianBroken/Qwen_Fine-tuning.git\n",
        "%cd Qwen_Fine-tuning\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "mlECHOcnznjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Ajusta aquÃ­ el nombre del subdirectorio donde quieres guardar tu adaptador:\n",
        "output_subdir = \"output/qwen-ft\"\n",
        "\n",
        "# Construye la ruta absoluta y crÃ©ala si no existe\n",
        "adapter_path = Path(output_subdir)\n",
        "adapter_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Convierte a cadena para usar en el comando\n",
        "adapter_path = str(adapter_path)\n",
        "\n",
        "print(f\"âœ… Ruta de salida creada en: {adapter_path}\")"
      ],
      "metadata": {
        "id": "NqsjnjgdzfQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda: Combinar datasets al formato de \"messages\"\n",
        "import json\n",
        "\n",
        "def convert_formateado(line):\n",
        "    rec = json.loads(line)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",    \"content\": \"Eres un asistente Ãºtil.\"},\n",
        "            {\"role\": \"user\",      \"content\": rec[\"title\"]},\n",
        "            {\"role\": \"assistant\", \"content\": rec[\"description\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def convert_rare(line):\n",
        "    rec = json.loads(line)\n",
        "    # Construir el contenido de usuario a partir de instruction e input\n",
        "    user_content = rec[\"instruction\"]\n",
        "    if rec.get(\"input\"):\n",
        "        # Si hay input, lo concatenamos en nueva lÃ­nea\n",
        "        user_content += \"\\n\" + rec[\"input\"]\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",    \"content\": \"Eres un asistente Ãºtil.\"},\n",
        "            {\"role\": \"user\",      \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": rec[\"output\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Archivos de entrada y salida\n",
        "input_files = [\n",
        "    (\"/content/Qwen_Fine-tuning/datos-instruct-formateado.jsonl\", convert_formateado),\n",
        "    (\"/content/Qwen_Fine-tuning/rare_instruction_finetune.jsonl\", convert_rare)\n",
        "]\n",
        "output_path = \"combined_dataset.jsonl\"\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for fname, converter in input_files:\n",
        "        with open(fname, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                new_rec = converter(line)\n",
        "                fout.write(json.dumps(new_rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"âœ… Dataset combinado guardado en `{output_path}`\")\n"
      ],
      "metadata": {
        "id": "rVXb0l6TwyQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notebook_intro_markdown"
      },
      "source": [
        "# ğŸ”§ Ajuste fino de Qwen con EchoHeart en Google Colab\n",
        "\n",
        "Este notebook configura automÃ¡ticamente el entorno, inicia el entrenamiento de ajuste fino (QLoRA), realiza pruebas, fusiona el adaptador LoRA con el modelo base y exporta el modelo final en formato GGUF.\n",
        "\n",
        "### ğŸ“Œ ConfiguraciÃ³n\n",
        "Para comenzar, especifica el modelo base y el conjunto de datos modificando las variables en el primer bloque de cÃ³digo mÃ¡s abajo.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸªœ Pasos del proceso\n",
        "\n",
        "1. âœ… **Configurar variables, clonar o actualizar el repositorio de GitHub y definir rutas.**  \n",
        "2. ğŸ“¦ **Instalar las dependencias necesarias.**  \n",
        "3. ğŸ§  **Ejecutar el script de entrenamiento con QLoRA.**  \n",
        "4. ğŸ§ª **(Opcional)** Ejecutar un script de prueba para interactuar con el modelo afinado (adaptador).  \n",
        "5. ğŸ”€ **Fusionar el adaptador LoRA con el modelo base.**  \n",
        "6. ğŸ“¤ **(Opcional)** Exportar el modelo fusionado al formato GGUF para despliegue eficiente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title ğŸ”§ ConfiguraciÃ³n principal y parÃ¡metros de entrenamiento\n",
        "#@markdown ### âš™ï¸ Modelo y archivo de datos\n",
        "base_model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  #@param {type:\"string\"}\n",
        "#@markdown *Nombre o ruta del modelo base en Hugging Face (por ejemplo: `\"Qwen/Qwen2.5-7B-Instruct\"`)*\n",
        "\n",
        "dataset_file: str = \"/content/Qwen_Fine-tuning/combined_dataset.jsonl\"  #@param {type:\"string\"}\n",
        "#@markdown *Ruta relativa al archivo JSONL del dataset a usar para el fine-tuning*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ§ª HiperparÃ¡metros de entrenamiento\n",
        "num_train_epochs: int = 2  #@param {type:\"integer\"}\n",
        "#@markdown *NÃºmero de Ã©pocas (vueltas completas al dataset)*\n",
        "\n",
        "learning_rate: float = 0.0002  #@param {type:\"number\"}\n",
        "#@markdown *Tasa de aprendizaje inicial*\n",
        "\n",
        "weight_decay: float = 0.01  #@param {type:\"number\"}\n",
        "#@markdown *Factor de decaimiento del peso (weight decay) para evitar overfitting*\n",
        "\n",
        "max_grad_norm: float = 1.0  #@param {type:\"number\"}\n",
        "#@markdown *LÃ­mite al valor de los gradientes (grad clipping)*\n",
        "\n",
        "seed: int = 42  #@param {type:\"integer\"}\n",
        "#@markdown *Semilla aleatoria para reproducibilidad*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ§© ParÃ¡metros de LoRA\n",
        "lora_r: int = 32  #@param {type:\"integer\"}\n",
        "#@markdown *Rango (rank) para LoRA*\n",
        "\n",
        "lora_alpha: int = 64  #@param {type:\"integer\"}\n",
        "#@markdown *Alpha (factor de escalado) para LoRA*\n",
        "\n",
        "lora_dropout: float = 0.05  #@param {type:\"number\"}\n",
        "#@markdown *Dropout aplicado dentro de LoRA*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ’¾ Guardado y registros\n",
        "save_steps: int = 25  #@param {type:\"integer\"}\n",
        "#@markdown *Guardar un checkpoint cada N pasos*\n",
        "\n",
        "logging_steps: int = 5  #@param {type:\"integer\"}\n",
        "#@markdown *Registrar informaciÃ³n cada N pasos*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ—‚ï¸ Ruta personalizada de salida (opcional)\n",
        "custom_output_dir: str = \"\"  #@param {type:\"string\"}\n",
        "#@markdown *Ruta relativa personalizada para guardar el adaptador entrenado (dejar vacÃ­o para generar automÃ¡ticamente)*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### ğŸ“ Directorio de trabajo base\n",
        "target_workspace_dir: str = \"/content\"  #@param {type:\"string\"}\n",
        "#@markdown *Directorio base donde se clonarÃ¡ el repositorio (en Colab se ajusta automÃ¡ticamente a `/content`)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCd-DEGuXUJ6"
      },
      "outputs": [],
      "source": [
        "# 3. Ejecutar script de entrenamiento (QLoRA)\n",
        "print(\"Starting QLoRA training...\")\n",
        "\n",
        "train_command = (\n",
        "    f\"python train.py \"\n",
        "    f\"--base_model_name \\\"{base_model_name}\\\" \"\n",
        "    f\"--dataset_file \\\"{dataset_file}\\\" \"\n",
        "    f\"--output_dir \\\"{adapter_path}\\\" \"\n",
        "    f\"--num_train_epochs {num_train_epochs} \"\n",
        "    f\"--learning_rate {learning_rate} \"\n",
        "    f\"--weight_decay {weight_decay} \"\n",
        "    f\"--max_grad_norm {max_grad_norm} \"\n",
        "    f\"--lora_r {lora_r} \"\n",
        "    f\"--lora_alpha {lora_alpha} \"\n",
        "    f\"--lora_dropout {lora_dropout} \"\n",
        "    f\"--save_steps {save_steps} \"\n",
        "    f\"--logging_steps {logging_steps} \"\n",
        "    f\"--seed {seed}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- Running Training Command ---\")\n",
        "print(train_command)\n",
        "print(\"------------------------------\\n\")\n",
        "\n",
        "# Ejecutar comando\n",
        "!{train_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFKaIkLXUJ6"
      },
      "source": [
        "## Â¡Entrenamiento completado!\n",
        "\n",
        "El modelo ajustado se guarda en el directorio `output/qwen-ft` del sistema de archivos del entorno Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "added_test_step"
      },
      "outputs": [],
      "source": [
        "# 4. è¿è¡Œæµ‹è¯•è„šæœ¬\n",
        "print(\"Starting non-interactive testing session...\")\n",
        "!python test_model.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gguf_markdown"
      },
      "source": [
        "## 5. (Opcional) Exportar al formato GGUF\n",
        "\n",
        "El archivo convertido se guardarÃ¡ en la ruta: `{gguf_output_file}` (segÃºn tu configuraciÃ³n).\n",
        "\n",
        "Este formato es Ãºtil para despliegue eficiente en entornos ligeros como servidores locales o integraciones con motores de inferencia optimizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model_path = \"merged/\"\n",
        "gguf_output_abs_path = \"/content/merged.gguf\""
      ],
      "metadata": {
        "id": "cZ5OhajMGL2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "added_gguf_step"
      },
      "outputs": [],
      "source": [
        "# è¿è¡Œ LoRA åˆå¹¶è„šæœ¬\n",
        "#print(\"Starting LoRA merge...\")\n",
        "#!python merge_lora.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\" --output_path \"{merged_model_path}\"\n",
        "\n",
        "# è½¬æ¢ä¸º GGUF\n",
        "print(\"Starting GGUF conversion...\")\n",
        "!python convert_to_gguf.py --model_dir \"/content/Qwen_Fine-tuning/merged\" --output_file \"{gguf_output_abs_path}\" --out_type f16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xw10E_7qEbvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Qwen_Fine-tuning/app_gradio_qwen_ft.py"
      ],
      "metadata": {
        "id": "41iWLw_eElim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_103Y8IoaEg"
      },
      "source": [
        "## (å¯é€‰) æŸ¥çœ‹ train.py çš„é«˜çº§å‚æ•°\n",
        "\n",
        "è¿è¡Œä¸‹é¢çš„ä»£ç å•å…ƒæ ¼å¯ä»¥æ˜¾ç¤º `train.py` è„šæœ¬æ”¯æŒçš„æ‰€æœ‰å‘½ä»¤è¡Œå‚æ•°åŠå…¶è¯´æ˜å’Œé»˜è®¤å€¼ã€‚\n",
        "å¦‚æœæ‚¨æƒ³è¦†ç›–é»˜è®¤è®¾ç½®ï¼ˆä¾‹å¦‚è°ƒæ•´å­¦ä¹ ç‡ã€LoRA rankã€ä¿å­˜æ­¥æ•°ç­‰ï¼‰ï¼Œå¯ä»¥åœ¨ç¬¬ 3 æ­¥è¿è¡Œè®­ç»ƒæ—¶æ‰‹åŠ¨æ·»åŠ è¿™äº›å‚æ•°ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48FQ9m_ToaEg"
      },
      "outputs": [],
      "source": [
        "!python train.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-LTXv9oaEg"
      },
      "source": [
        "# 6. (å¯é€‰) è¿è¡Œ GGUF æ¨¡å‹æµ‹è¯•è„šæœ¬\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkNDgNMEoaEg"
      },
      "outputs": [],
      "source": [
        "# æ£€æŸ¥ GGUF æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "import os\n",
        "if os.path.exists(gguf_output_abs_path):\n",
        "    print(f\"GGUF æ–‡ä»¶å­˜åœ¨: {gguf_output_abs_path}\")\n",
        "    print(\"è¿è¡Œ GGUF æ¨¡å‹æµ‹è¯•...\")\n",
        "\n",
        "    print(\"å®‰è£… llama-cpp-python...\")\n",
        "    # ä½¿ç”¨ CMAKE_ARGS æ¥å¯ç”¨ cuBLAS ä»¥æ”¯æŒ GPU åŠ é€Ÿ\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python\n",
        "    !pip install -q llama-cpp-python\n",
        "\n",
        "    # æ„å»ºæµ‹è¯•å‘½ä»¤\n",
        "    test_gguf_command = (\n",
        "        f\"python test_gguf.py \"\n",
        "        f\"--gguf_model_path \\\\\\\"{gguf_output_abs_path}\\\\\\\" \"\n",
        "        f\"--n_gpu_layers -1 \" # å°è¯•ä½¿ç”¨æ‰€æœ‰ GPU å±‚\n",
        "        f\"--chat_format qwen\" # æ˜ç¡®æŒ‡å®š Qwen æ ¼å¼\n",
        "    )\n",
        "\n",
        "    print(\"\\\\n--- Running GGUF Test Command ---\")\n",
        "    print(test_gguf_command)\n",
        "    print(\"---------------------------------\\\\n\")\n",
        "\n",
        "    # æ‰§è¡Œæµ‹è¯•å‘½ä»¤\n",
        "    !{test_gguf_command}\n",
        "\n",
        "else:\n",
        "    print(f\"é”™è¯¯ï¼šGGUF æ–‡ä»¶æœªæ‰¾åˆ°: {gguf_output_abs_path}\")\n",
        "    print(\"è¯·ç¡®ä¿å‰é¢çš„ GGUF è½¬æ¢æ­¥éª¤å·²æˆåŠŸå®Œæˆã€‚\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "mjGrQM5SJ7U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# âš™ï¸ Ruta al modelo GGUF ya fusionado\n",
        "GGUF_MODEL_PATH = \"/content/merged.gguf\"\n",
        "\n",
        "# ğŸ”§ ParÃ¡metros de carga (ajusta segÃºn RAM/VRAM disponible)\n",
        "MODEL = Llama(\n",
        "    model_path=GGUF_MODEL_PATH,\n",
        "    n_ctx=2048,\n",
        "    n_threads=os.cpu_count(),\n",
        "    n_gpu_layers=33,  # usa 0 si no tienes GPU compatible, o ajusta a tu VRAM (ej: 33 para 8GB)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# ğŸ’¬ GeneraciÃ³n de respuesta\n",
        "def chat_with_model(prompt, temperature=0.7, max_tokens=256, top_p=0.95, repeat_penalty=1.15):\n",
        "    try:\n",
        "        # Si tu modelo usa template tipo ChatML (Qwen), agrega instrucciÃ³n como:\n",
        "        formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        output = MODEL(\n",
        "            formatted_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "            stop=[\"<|im_end|>\"]\n",
        "        )\n",
        "        return output[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        return f\"âŒ Error: {e}\"\n",
        "\n",
        "# ğŸ§ª Interfaz Gradio\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_model,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=4, placeholder=\"Escribe algo...\", label=\"Tu mensaje\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=64, maximum=1024, value=256, label=\"Max Tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, label=\"Top-p\"),\n",
        "        gr.Slider(minimum=1.0, maximum=2.0, value=1.15, label=\"PenalizaciÃ³n de repeticiÃ³n\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"ğŸ§  Chat GGUF - Qwen Fine-tuned\",\n",
        "    description=\"InteractÃºa con tu modelo GGUF convertido desde QLoRA con Gradio\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "# ğŸŸ¢ Ejecutar app\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "aB_oVRjeKCL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}