{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Filarh/Cringe-scraper/blob/main/run_in_colab_traducido.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/NianBroken/Qwen_Fine-tuning.git\n",
        "%cd Qwen_Fine-tuning\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "mlECHOcnznjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Ajusta aqu√≠ el nombre del subdirectorio donde quieres guardar tu adaptador:\n",
        "output_subdir = \"output/qwen-ft\"\n",
        "\n",
        "# Construye la ruta absoluta y cr√©ala si no existe\n",
        "adapter_path = Path(output_subdir)\n",
        "adapter_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Convierte a cadena para usar en el comando\n",
        "adapter_path = str(adapter_path)\n",
        "\n",
        "print(f\"‚úÖ Ruta de salida creada en: {adapter_path}\")"
      ],
      "metadata": {
        "id": "NqsjnjgdzfQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Celda: Combinar datasets al formato de \"messages\"\n",
        "import json\n",
        "\n",
        "def convert_formateado(line):\n",
        "    rec = json.loads(line)\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",    \"content\": \"Eres un asistente √∫til.\"},\n",
        "            {\"role\": \"user\",      \"content\": rec[\"title\"]},\n",
        "            {\"role\": \"assistant\", \"content\": rec[\"description\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "def convert_rare(line):\n",
        "    rec = json.loads(line)\n",
        "    # Construir el contenido de usuario a partir de instruction e input\n",
        "    user_content = rec[\"instruction\"]\n",
        "    if rec.get(\"input\"):\n",
        "        # Si hay input, lo concatenamos en nueva l√≠nea\n",
        "        user_content += \"\\n\" + rec[\"input\"]\n",
        "    return {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",    \"content\": \"Eres un asistente √∫til.\"},\n",
        "            {\"role\": \"user\",      \"content\": user_content},\n",
        "            {\"role\": \"assistant\", \"content\": rec[\"output\"]}\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Archivos de entrada y salida\n",
        "input_files = [\n",
        "    (\"/content/Qwen_Fine-tuning/datos-instruct-formateado.jsonl\", convert_formateado),\n",
        "    (\"/content/Qwen_Fine-tuning/rare_instruction_finetune.jsonl\", convert_rare)\n",
        "]\n",
        "output_path = \"combined_dataset.jsonl\"\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "    for fname, converter in input_files:\n",
        "        with open(fname, \"r\", encoding=\"utf-8\") as fin:\n",
        "            for line in fin:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                new_rec = converter(line)\n",
        "                fout.write(json.dumps(new_rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"‚úÖ Dataset combinado guardado en `{output_path}`\")\n"
      ],
      "metadata": {
        "id": "rVXb0l6TwyQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notebook_intro_markdown"
      },
      "source": [
        "# üîß Ajuste fino de Qwen con EchoHeart en Google Colab\n",
        "\n",
        "Este notebook configura autom√°ticamente el entorno, inicia el entrenamiento de ajuste fino (QLoRA), realiza pruebas, fusiona el adaptador LoRA con el modelo base y exporta el modelo final en formato GGUF.\n",
        "\n",
        "### üìå Configuraci√≥n\n",
        "Para comenzar, especifica el modelo base y el conjunto de datos modificando las variables en el primer bloque de c√≥digo m√°s abajo.\n",
        "\n",
        "---\n",
        "\n",
        "### ü™ú Pasos del proceso\n",
        "\n",
        "1. ‚úÖ **Configurar variables, clonar o actualizar el repositorio de GitHub y definir rutas.**  \n",
        "2. üì¶ **Instalar las dependencias necesarias.**  \n",
        "3. üß† **Ejecutar el script de entrenamiento con QLoRA.**  \n",
        "4. üß™ **(Opcional)** Ejecutar un script de prueba para interactuar con el modelo afinado (adaptador).  \n",
        "5. üîÄ **Fusionar el adaptador LoRA con el modelo base.**  \n",
        "6. üì§ **(Opcional)** Exportar el modelo fusionado al formato GGUF para despliegue eficiente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps_cell",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title üîß Configuraci√≥n principal y par√°metros de entrenamiento\n",
        "#@markdown ### ‚öôÔ∏è Modelo y archivo de datos\n",
        "base_model_name: str = \"Qwen/Qwen2.5-1.5B-Instruct\"  #@param {type:\"string\"}\n",
        "#@markdown *Nombre o ruta del modelo base en Hugging Face (por ejemplo: `\"Qwen/Qwen2.5-7B-Instruct\"`)*\n",
        "\n",
        "dataset_file: str = \"/content/Qwen_Fine-tuning/combined_dataset.jsonl\"  #@param {type:\"string\"}\n",
        "#@markdown *Ruta relativa al archivo JSONL del dataset a usar para el fine-tuning*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß™ Hiperpar√°metros de entrenamiento\n",
        "num_train_epochs: int = 2  #@param {type:\"integer\"}\n",
        "#@markdown *N√∫mero de √©pocas (vueltas completas al dataset)*\n",
        "\n",
        "learning_rate: float = 0.0002  #@param {type:\"number\"}\n",
        "#@markdown *Tasa de aprendizaje inicial*\n",
        "\n",
        "weight_decay: float = 0.01  #@param {type:\"number\"}\n",
        "#@markdown *Factor de decaimiento del peso (weight decay) para evitar overfitting*\n",
        "\n",
        "max_grad_norm: float = 1.0  #@param {type:\"number\"}\n",
        "#@markdown *L√≠mite al valor de los gradientes (grad clipping)*\n",
        "\n",
        "seed: int = 42  #@param {type:\"integer\"}\n",
        "#@markdown *Semilla aleatoria para reproducibilidad*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üß© Par√°metros de LoRA\n",
        "lora_r: int = 32  #@param {type:\"integer\"}\n",
        "#@markdown *Rango (rank) para LoRA*\n",
        "\n",
        "lora_alpha: int = 64  #@param {type:\"integer\"}\n",
        "#@markdown *Alpha (factor de escalado) para LoRA*\n",
        "\n",
        "lora_dropout: float = 0.05  #@param {type:\"number\"}\n",
        "#@markdown *Dropout aplicado dentro de LoRA*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üíæ Guardado y registros\n",
        "save_steps: int = 25  #@param {type:\"integer\"}\n",
        "#@markdown *Guardar un checkpoint cada N pasos*\n",
        "\n",
        "logging_steps: int = 5  #@param {type:\"integer\"}\n",
        "#@markdown *Registrar informaci√≥n cada N pasos*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üóÇÔ∏è Ruta personalizada de salida (opcional)\n",
        "custom_output_dir: str = \"\"  #@param {type:\"string\"}\n",
        "#@markdown *Ruta relativa personalizada para guardar el adaptador entrenado (dejar vac√≠o para generar autom√°ticamente)*\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ### üìÅ Directorio de trabajo base\n",
        "target_workspace_dir: str = \"/content\"  #@param {type:\"string\"}\n",
        "#@markdown *Directorio base donde se clonar√° el repositorio (en Colab se ajusta autom√°ticamente a `/content`)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCd-DEGuXUJ6"
      },
      "outputs": [],
      "source": [
        "# 3. Ejecutar script de entrenamiento (QLoRA)\n",
        "print(\"Starting QLoRA training...\")\n",
        "\n",
        "train_command = (\n",
        "    f\"python train.py \"\n",
        "    f\"--base_model_name \\\"{base_model_name}\\\" \"\n",
        "    f\"--dataset_file \\\"{dataset_file}\\\" \"\n",
        "    f\"--output_dir \\\"{adapter_path}\\\" \"\n",
        "    f\"--num_train_epochs {num_train_epochs} \"\n",
        "    f\"--learning_rate {learning_rate} \"\n",
        "    f\"--weight_decay {weight_decay} \"\n",
        "    f\"--max_grad_norm {max_grad_norm} \"\n",
        "    f\"--lora_r {lora_r} \"\n",
        "    f\"--lora_alpha {lora_alpha} \"\n",
        "    f\"--lora_dropout {lora_dropout} \"\n",
        "    f\"--save_steps {save_steps} \"\n",
        "    f\"--logging_steps {logging_steps} \"\n",
        "    f\"--seed {seed}\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- Running Training Command ---\")\n",
        "print(train_command)\n",
        "print(\"------------------------------\\n\")\n",
        "\n",
        "# Ejecutar comando\n",
        "!{train_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GFKaIkLXUJ6"
      },
      "source": [
        "## ¬°Entrenamiento completado!\n",
        "\n",
        "El modelo ajustado se guarda en el directorio `output/qwen-ft` del sistema de archivos del entorno Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "added_test_step"
      },
      "outputs": [],
      "source": [
        "# 4. ËøêË°åÊµãËØïËÑöÊú¨\n",
        "print(\"Starting non-interactive testing session...\")\n",
        "!python test_model.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gguf_markdown"
      },
      "source": [
        "## 5. (Opcional) Exportar al formato GGUF\n",
        "\n",
        "El archivo convertido se guardar√° en la ruta: `{gguf_output_file}` (seg√∫n tu configuraci√≥n).\n",
        "\n",
        "Este formato es √∫til para despliegue eficiente en entornos ligeros como servidores locales o integraciones con motores de inferencia optimizados.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model_path = \"merged/\"\n",
        "gguf_output_abs_path = \"/content/merged.gguf\""
      ],
      "metadata": {
        "id": "cZ5OhajMGL2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "added_gguf_step"
      },
      "outputs": [],
      "source": [
        "# ËøêË°å LoRA ÂêàÂπ∂ËÑöÊú¨\n",
        "#print(\"Starting LoRA merge...\")\n",
        "#!python merge_lora.py --base_model_name \"{base_model_name}\" --adapter_path \"{adapter_path}\" --output_path \"{merged_model_path}\"\n",
        "\n",
        "# ËΩ¨Êç¢‰∏∫ GGUF\n",
        "print(\"Starting GGUF conversion...\")\n",
        "!python convert_to_gguf.py --model_dir \"/content/Qwen_Fine-tuning/merged\" --output_file \"{gguf_output_abs_path}\" --out_type f16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xw10E_7qEbvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/Qwen_Fine-tuning/app_gradio_qwen_ft.py"
      ],
      "metadata": {
        "id": "41iWLw_eElim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_103Y8IoaEg"
      },
      "source": [
        "## (ÂèØÈÄâ) Êü•Áúã train.py ÁöÑÈ´òÁ∫ßÂèÇÊï∞\n",
        "\n",
        "ËøêË°å‰∏ãÈù¢ÁöÑ‰ª£Á†ÅÂçïÂÖÉÊ†ºÂèØ‰ª•ÊòæÁ§∫ `train.py` ËÑöÊú¨ÊîØÊåÅÁöÑÊâÄÊúâÂëΩ‰ª§Ë°åÂèÇÊï∞ÂèäÂÖ∂ËØ¥ÊòéÂíåÈªòËÆ§ÂÄº„ÄÇ\n",
        "Â¶ÇÊûúÊÇ®ÊÉ≥Ë¶ÜÁõñÈªòËÆ§ËÆæÁΩÆÔºà‰æãÂ¶ÇË∞ÉÊï¥Â≠¶‰π†Áéá„ÄÅLoRA rank„ÄÅ‰øùÂ≠òÊ≠•Êï∞Á≠âÔºâÔºåÂèØ‰ª•Âú®Á¨¨ 3 Ê≠•ËøêË°åËÆ≠ÁªÉÊó∂ÊâãÂä®Ê∑ªÂä†Ëøô‰∫õÂèÇÊï∞„ÄÇ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48FQ9m_ToaEg"
      },
      "outputs": [],
      "source": [
        "!python train.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY-LTXv9oaEg"
      },
      "source": [
        "# 6. (ÂèØÈÄâ) ËøêË°å GGUF Ê®°ÂûãÊµãËØïËÑöÊú¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkNDgNMEoaEg"
      },
      "outputs": [],
      "source": [
        "# Ê£ÄÊü• GGUF Êñá‰ª∂ÊòØÂê¶Â≠òÂú®\n",
        "import os\n",
        "if os.path.exists(gguf_output_abs_path):\n",
        "    print(f\"GGUF Êñá‰ª∂Â≠òÂú®: {gguf_output_abs_path}\")\n",
        "    print(\"ËøêË°å GGUF Ê®°ÂûãÊµãËØï...\")\n",
        "\n",
        "    print(\"ÂÆâË£Ö llama-cpp-python...\")\n",
        "    # ‰ΩøÁî® CMAKE_ARGS Êù•ÂêØÁî® cuBLAS ‰ª•ÊîØÊåÅ GPU Âä†ÈÄü\n",
        "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python\n",
        "    !pip install -q llama-cpp-python\n",
        "\n",
        "    # ÊûÑÂª∫ÊµãËØïÂëΩ‰ª§\n",
        "    test_gguf_command = (\n",
        "        f\"python test_gguf.py \"\n",
        "        f\"--gguf_model_path \\\\\\\"{gguf_output_abs_path}\\\\\\\" \"\n",
        "        f\"--n_gpu_layers -1 \" # Â∞ùËØï‰ΩøÁî®ÊâÄÊúâ GPU Â±Ç\n",
        "        f\"--chat_format qwen\" # ÊòéÁ°ÆÊåáÂÆö Qwen Ê†ºÂºè\n",
        "    )\n",
        "\n",
        "    print(\"\\\\n--- Running GGUF Test Command ---\")\n",
        "    print(test_gguf_command)\n",
        "    print(\"---------------------------------\\\\n\")\n",
        "\n",
        "    # ÊâßË°åÊµãËØïÂëΩ‰ª§\n",
        "    !{test_gguf_command}\n",
        "\n",
        "else:\n",
        "    print(f\"ÈîôËØØÔºöGGUF Êñá‰ª∂Êú™ÊâæÂà∞: {gguf_output_abs_path}\")\n",
        "    print(\"ËØ∑Á°Æ‰øùÂâçÈù¢ÁöÑ GGUF ËΩ¨Êç¢Ê≠•È™§Â∑≤ÊàêÂäüÂÆåÊàê„ÄÇ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "mjGrQM5SJ7U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# ‚öôÔ∏è Ruta al modelo GGUF ya fusionado\n",
        "GGUF_MODEL_PATH = \"/content/merged.gguf\"\n",
        "\n",
        "# üîß Par√°metros de carga (ajusta seg√∫n RAM/VRAM disponible)\n",
        "MODEL = Llama(\n",
        "    model_path=GGUF_MODEL_PATH,\n",
        "    n_ctx=2048,\n",
        "    n_threads=os.cpu_count(),\n",
        "    n_gpu_layers=33,  # usa 0 si no tienes GPU compatible, o ajusta a tu VRAM (ej: 33 para 8GB)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# üí¨ Generaci√≥n de respuesta\n",
        "def chat_with_model(prompt, temperature=0.7, max_tokens=256, top_p=0.95, repeat_penalty=1.15):\n",
        "    try:\n",
        "        # Si tu modelo usa template tipo ChatML (Qwen), agrega instrucci√≥n como:\n",
        "        formatted_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "        output = MODEL(\n",
        "            formatted_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repeat_penalty=repeat_penalty,\n",
        "            stop=[\"<|im_end|>\"]\n",
        "        )\n",
        "        return output[\"choices\"][0][\"text\"].strip()\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {e}\"\n",
        "\n",
        "# üß™ Interfaz Gradio\n",
        "interface = gr.Interface(\n",
        "    fn=chat_with_model,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=4, placeholder=\"Escribe algo...\", label=\"Tu mensaje\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.5, value=0.7, label=\"Temperature\"),\n",
        "        gr.Slider(minimum=64, maximum=1024, value=256, label=\"Max Tokens\"),\n",
        "        gr.Slider(minimum=0.1, maximum=1.0, value=0.95, label=\"Top-p\"),\n",
        "        gr.Slider(minimum=1.0, maximum=2.0, value=1.15, label=\"Penalizaci√≥n de repetici√≥n\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üß† Chat GGUF - Qwen Fine-tuned\",\n",
        "    description=\"Interact√∫a con tu modelo GGUF convertido desde QLoRA con Gradio\",\n",
        "    theme=\"default\"\n",
        ")\n",
        "\n",
        "# üü¢ Ejecutar app\n",
        "if __name__ == \"__main__\":\n",
        "    interface.launch(share=True)\n"
      ],
      "metadata": {
        "id": "aB_oVRjeKCL_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}